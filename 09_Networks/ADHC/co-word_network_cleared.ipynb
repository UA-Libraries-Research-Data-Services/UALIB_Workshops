{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Visualization of Chronicling America Newspapers\n",
    "\n",
    "**Avery Fernandez and Vincent Scalfani**\n",
    "\n",
    "UA Libraries, Data Services\n",
    "\n",
    "April 19, 2023\n",
    "\n",
    "Code in this notebook is MIT licensed, you can find a copy of the license here: https://github.com/ualibweb/UALIB_Workshops\n",
    "\n",
    "Some code in this workshop has been adapted from :\n",
    "https://github.com/ualibweb/UALIB_Workshops/tree/master/09_Networks\n",
    "\n",
    "https://ualibweb.github.io/UALIB_ScholarlyAPI_Cookbook/content/scripts/python/python_chronam.html\n",
    "\n",
    "\n",
    "Chronicling America data is credited to Library of Congress. Please see https://chroniclingamerica.loc.gov/about/api/ for Data Usage Policies and Disclaimers.\n",
    "\n",
    "NetworkX Docs: https://networkx.org/documentation/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Following Along\n",
    "\n",
    "```\n",
    "conda create --name my-env\n",
    "conda activate my-env\n",
    "conda install -c conda-forge jupyterlab numpy matplotlib pandas networkx\n",
    "```\n",
    "or through pip:\n",
    "\n",
    "``` \n",
    "pip install numpy matplotlib pandas networkx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Workflow\n",
    "\n",
    "1. Use the Chronicling America API to obtain some full-text newspaper data. In this example, we will\n",
    "search for srticles related to \"University of Alabama\".\n",
    "\n",
    "2. Cleanup some of the obtained OCR text:\n",
    "\n",
    "   * Converts the text to lowercase.\n",
    "   * Removes invalid characters (characters other than letters, digits, punctuation marks, and whitespace).\n",
    "   * Removes excess spaces.\n",
    "   * Splits the text by punctuation marks (., !, ?) and removes them.\n",
    "   * Removes any empty strings from the resulting list of sentences.\n",
    "   \n",
    "3. Compute combinations and occurances of co-words\n",
    "\n",
    "4. Remove occurances of common, \"uninteresting\" word pairs\n",
    "\n",
    "5. Create a basic network of co-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Basic Chronicling America API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API base URL\n",
    "api = \"https://chroniclingamerica.loc.gov/\"\n",
    "\n",
    "# Construct the API request URL\n",
    "# This request will search for articles related to \"University of Alabama\" in the state of Alabama\n",
    "# It will return the top 10 results in JSON format\n",
    "request_url = (api + \"search/pages/results/?state=Alabama&andtext=(University+of+Alabama)\"\n",
    "               \"&rows=10&format=json\")\n",
    "\n",
    "# Send the API request and parse the JSON response\n",
    "request = requests.get(request_url).json()\n",
    "\n",
    "# Print the 'ocr_eng' field of the first item in the response\n",
    "# The 'ocr_eng' field contains the OCR text of the newspaper page\n",
    "pprint(request['items'][0]['ocr_eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save JSON data to a file\n",
    "with open('example_request.json', 'w') as outfile:\n",
    "    json.dump(request, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON data from a file\n",
    "#with open('example_request.json','r') as infile:\n",
    "#    loadedData = json.load(infile)\n",
    "#pprint(loadedData['items'][0]['ocr_eng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get Chronicling America OCR Text in a Loop\n",
    "\n",
    "If you try and request too many rows at a time, the Chronicling America API is sensitive to timing out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API base URL\n",
    "api = \"https://chroniclingamerica.loc.gov/\"\n",
    "\n",
    "# Initialize the list to store OCR text data\n",
    "ocr_eng_data = []\n",
    "\n",
    "# Set the number of articles you want to fetch\n",
    "articles_to_fetch = 2000\n",
    "\n",
    "# Calculate the number of API calls required\n",
    "api_calls_required = articles_to_fetch // 100\n",
    "print(api_calls_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles in chunks of 100\n",
    "for i in range(api_calls_required):\n",
    "    # Define search parameters for the API request\n",
    "    search_params = {\n",
    "        \"state\": \"Alabama\",\n",
    "        \"andtext\": \"(University of Alabama)\",\n",
    "        \"rows\": 100,\n",
    "        \"page\": i + 1,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    # Send the API request and parse the JSON response\n",
    "    request = requests.get(api + \"search/pages/results/\", params=search_params).json()\n",
    "\n",
    "    # Wait for 1 second between API calls to avoid overloading the server\n",
    "    sleep(1)\n",
    "\n",
    "    # Extract 'ocr_eng' data and append to the list of lists\n",
    "    ocr_eng_data.append([item['ocr_eng'] for item in request['items']])\n",
    "    \n",
    "# Flatten the list of lists to a single list\n",
    "ocr_eng_data_flat = [ocr_eng for sublist in ocr_eng_data for ocr_eng in sublist]\n",
    "\n",
    "# This is the same , but not in list comprehension\n",
    "#ocr_eng_data_flat = []\n",
    "#for sublist in ocr_eng_data:\n",
    "#    for ocr_engine in sublist:\n",
    "#        ocr_eng_data_flat.append(ocr_engine)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as a pickle\n",
    "import pickle\n",
    "\n",
    "with open('ocr_eng_data.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ocr_eng_data, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('ocr_eng_data_flat.pickle', 'wb') as outfile:\n",
    "    pickle.dump(ocr_eng_data_flat, outfile, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load pickle data\n",
    "\n",
    "#import pickle\n",
    "#with open('ocr_eng_data_flat.pickle', 'rb') as infile:\n",
    "#    loaded_ocr_eng_data_flat = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 'ocr_eng' data items\n",
    "for i, ocr_eng in enumerate(ocr_eng_data_flat[:2]):\n",
    "    print(f\"Article {i + 1}:\")\n",
    "    print(ocr_eng)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function clean_and_split_text takes a text string as input and performs the following tasks:\n",
    "\n",
    "1. Converts the text to lowercase.\n",
    "2. Removes invalid characters (characters other than letters, digits, punctuation marks, and whitespace).\n",
    "3. Removes excess spaces.\n",
    "4. Splits the text by punctuation marks (., !, ?) and removes them.\n",
    "5. Removes any empty strings from the resulting list of sentences.\n",
    "\n",
    "The function is then used to process the ocr_eng_data_flat list, resulting in a new list called ocr_eng_sentences. The first 10 cleaned and split text items can be printed by uncommenting the last block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_split_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove invalid characters\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]+\", \"\", text)\n",
    "    \n",
    "    # Remove excess spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Split text by punctuation and remove it\n",
    "    sentences = re.split(r'[.!?]\\s*', cleaned_text)\n",
    "\n",
    "    # Remove any empty strings from the list\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Process the ocr_eng_data_flat list\n",
    "ocr_eng_sentences = [clean_and_split_text(ocr_eng) for ocr_eng in ocr_eng_data_flat]\n",
    "\n",
    "# # Print the first 10 cleaned and split text items\n",
    "# for i, sentences in enumerate(ocr_eng_sentences[:10]):\n",
    "#     print(f\"Article {i + 1}:\")\n",
    "#     print(sentences)\n",
    "#     print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet performs the following tasks:\n",
    "\n",
    "1. Initializes an empty list called word_pairs to store the combination pairs of words.\n",
    "2. Iterates through the list of cleaned and split sentences (ocr_eng_sentences) for each article.\n",
    "3. For each sentence, it splits the sentence into words.\n",
    "4. Generates word pairs (consecutive words) from the list of words and adds them to the word_pairs list.\n",
    "5. Finally, it prints the first 20 word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute combination pairs of words\n",
    "word_pairs = []\n",
    "for article_sentences in ocr_eng_sentences:\n",
    "    for sentence in article_sentences:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "        \n",
    "        # Generate word pairs and add them to the word_pairs list\n",
    "        word_pairs += [(words[word_idx], words[word_idx+1]) for word_idx in range(len(words)-1)]\n",
    "\n",
    "# Print the first 20 word pairs\n",
    "print(word_pairs[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet performs the following tasks:\n",
    "\n",
    "1. Initializes an empty dictionary called counted_pairs to store the occurrences of each word pair.\n",
    "2. Iterates through the list of word pairs (word_pairs).\n",
    "3. Checks if the current word pair is already in the dictionary counted_pairs. If so, it increments the count for the existing word pair by 1.\n",
    "4. If the word pair is not in the dictionary, it adds the word pair to the dictionary with an initial count of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression library\n",
    "import re\n",
    "\n",
    "# Count occurrences of word pairs\n",
    "counted_pairs = {}\n",
    "for pair in word_pairs:\n",
    "    # Remove any punctuation marks from the words in the pair\n",
    "    word1 = re.sub(r'[^\\w\\s]', '', pair[0])\n",
    "    word2 = re.sub(r'[^\\w\\s]', '', pair[1])\n",
    "    cleaned_pair = (word1, word2)\n",
    "    \n",
    "    # Check if the cleaned word pair is already in the dictionary\n",
    "    if cleaned_pair in counted_pairs.keys():\n",
    "        # Increment the count for the existing cleaned word pair\n",
    "        counted_pairs[cleaned_pair] += 1\n",
    "    else:\n",
    "        # Add a new cleaned word pair to the dictionary with an initial count of 1\n",
    "        counted_pairs[cleaned_pair] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(counted_pairs.items())[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs the following tasks:\n",
    "\n",
    "1. Defines a list of common words (common_words) to exclude from the filtered results.\n",
    "2. Creates a new dictionary called filtered_pairs that contains word pairs from the counted_pairs dictionary, filtered based on the following conditions:\n",
    "    * The first word of the pair (key[0]) should not be in the list of common words.\n",
    "    * The second word of the pair (key[1]) should not be in the list of common words.\n",
    "    * The occurrence count of the word pair (value) should be greater than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common words and numbers to exclude from the results\n",
    "common_words = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
    "    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "]\n",
    "\n",
    "# Filter word pairs by occurrence count and exclude common words and numbers\n",
    "filtered_pairs = {\n",
    "    key: value\n",
    "    for key, value in counted_pairs.items()\n",
    "    if key[0] not in common_words and key[1] not in common_words and value > 250\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the word pairs by highest frequency\n",
    "sorted_pairs = dict(sorted(filtered_pairs.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print the top 10 most frequent word pairs\n",
    "list(sorted_pairs.items())[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs the following tasks:\n",
    "\n",
    "1. Sorts the filtered_pairs dictionary by the occurrence count (value) in descending order. The key parameter in the sorted function is set to a lambda function that sorts the items by their values. The reverse parameter is set to True to sort the items in descending order. The sorted items are then used to create a new dictionary called sorted_pairs.\n",
    "2. Prints the top 10 most frequent word pairs by slicing the sorted items of the sorted_pairs dictionary and converting them back to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_pairs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in filtered_pairs.items():\n",
    "    if not (isinstance(pair, tuple) and len(pair) == 2 and\n",
    "            isinstance(pair[0], tuple) and len(pair[0]) == 2 and\n",
    "            all(isinstance(s, str) for s in pair[0]) and\n",
    "            isinstance(pair[1], int)):\n",
    "        raise ValueError(f\"Invalid value: {pair}\\n\"+ \"Invalid format: filtered_pairs must be a dictionary of tuples with the pattern {(String1, String2): Integer, ...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coword_network import Coword_Network\n",
    "networks = Coword_Network(filtered_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks.display_main_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks.display_focused_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks.sorted_graphs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks.display_selected_network(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "20503b97ed80c9e9db3f7b0dd2d260840fa40e15a8c80d40ec251c7c2e70c6e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
